{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1595398108963",
   "display_name": "Python 3.7.0 64-bit (virtualenv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More BeautifulSoup practice - select() , find() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "book_html=\"\"\"\n",
    "<ul id=\"bible\">\n",
    "<li id=\"ge\" class=\"first\">\n",
    "<ul class = \"second\">\n",
    "<li>aa</li>\n",
    "<li><span>cc</span>bb</li>\n",
    "</ul>\n",
    "</li>\n",
    "<li id=\"ex\" class=\"first\">나의첫사회생활</li>\n",
    "<li id=\"le\" class=\"second\">Still Me</li>\n",
    "<li id=\"nu\" class=\"second\">화염과 분노 : 도널드 트럼프의 백악관 뒷이야기</li>\n",
    "<li id=\"de\">매일 좋을 수만은 없는 여행을 한다</li>\n",
    "</ul>\n",
    "\"\"\"\n",
    "soup = BeautifulSoup(book_html, 'html.parser')\n",
    "\n",
    "# print(soup.prettify(formatter= 'html'))\n",
    "## These two are different\n",
    "# print('1 : ' ,soup.select(\"ul > li\"))\n",
    "# print('2 : ', soup.select(\"ul li\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* select(), select_one()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<class 'list'> [<li class=\"first\" id=\"ge\">\n<ul class=\"second\">\n<li>aa</li>\n<li><span>cc</span>bb</li>\n</ul>\n</li>, <li>aa</li>, <li><span>cc</span>bb</li>, <li class=\"first\" id=\"ex\">나의첫사회생활</li>, <li class=\"second\" id=\"le\">Still Me</li>, <li class=\"second\" id=\"nu\">화염과 분노 : 도널드 트럼프의 백악관 뒷이야기</li>, <li id=\"de\">매일 좋을 수만은 없는 여행을 한다</li>]\n----------------------\n[<ul class=\"second\">\n<li>aa</li>\n<li><span>cc</span>bb</li>\n</ul>] \n [<li class=\"second\" id=\"le\">Still Me</li>, <li class=\"second\" id=\"nu\">화염과 분노 : 도널드 트럼프의 백악관 뒷이야기</li>]\n"
    }
   ],
   "source": [
    "# select_ = soup.select('#ge')\n",
    "select_ =soup.select('li')\n",
    "# select_2 = soup.select('li#ge')\n",
    "print(type(select_), select_)\n",
    "print('----------------------')\n",
    "select_one = soup.select_one('#ge')\n",
    "# print(type(select_one), select_one , '\\n', select_one.li.string)\n",
    "\n",
    "select_3 = soup.select('ul.second')\n",
    "select_4 = soup.select('li.second')\n",
    "print(select_3 ,'\\n' ,  select_4)\n",
    "# for i in select_:\n",
    "    # print(i.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* find(), find_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<class 'bs4.element.Tag'> || <li class=\"first\" id=\"ge\">\n<ul class=\"second\">\n<li>aa</li>\n<li><span>cc</span>bb</li>\n</ul>\n</li>\n--------------------------\n<class 'bs4.element.ResultSet'> || [<li class=\"first\" id=\"ge\">\n<ul class=\"second\">\n<li>aa</li>\n<li><span>cc</span>bb</li>\n</ul>\n</li>, <li>aa</li>, <li><span>cc</span>bb</li>, <li class=\"first\" id=\"ex\">나의첫사회생활</li>, <li class=\"second\" id=\"le\">Still Me</li>, <li class=\"second\" id=\"nu\">화염과 분노 : 도널드 트럼프의 백악관 뒷이야기</li>, <li id=\"de\">매일 좋을 수만은 없는 여행을 한다</li>]\n--------------------------\n[<li class=\"second\" id=\"le\">Still Me</li>, <li class=\"second\" id=\"nu\">화염과 분노 : 도널드 트럼프의 백악관 뒷이야기</li>]\n--------------------------\nStill Me\n"
    }
   ],
   "source": [
    "find_ = soup.find('li')\n",
    "print(type(find_), '||', find_)\n",
    "print('--------------------------')\n",
    "find_all = soup.find_all('li')\n",
    "print(type(find_all), '||' , find_all)\n",
    "print('--------------------------')\n",
    "find_all_2 = soup.find_all('li', attrs={\"class\" : \"second\"}) \n",
    "print(find_all_2)\n",
    "print('--------------------------')\n",
    "find_2 = soup.find('li', attrs = {\"class\" : \"second\"})\n",
    "print(find_2.string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* lambda function : input - css selector | output - result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "나의첫사회생활\nStill Me\ncc\n[<span>cc</span>]\n[]\n----------------\n나의첫사회생활\n화염과 분노 : 도널드 트럼프의 백악관 뒷이야기\n"
    }
   ],
   "source": [
    "sel = lambda ex : print(soup.select_one(ex).string)\n",
    "\n",
    "sel('#ex')\n",
    "sel('ul li#le')\n",
    "\n",
    "###  '>' means, later one is right below the former one\n",
    "###  but white space can find even if it's not right below\n",
    "sel('ul.second  span')    \n",
    "#sel('ul.second > span')    ## this is error, since this is 'None'\n",
    "\n",
    "## these two are different\n",
    "print(soup.select('ul.second span'))\n",
    "print(soup.select('ul.second > span'))\n",
    "\n",
    "\n",
    "print('----------------')\n",
    "sel(\"li[id='ex']\")\n",
    "sel('li:nth-of-type(6)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[ All fruits ]\n사과\n포도\n레몬\n오렌지\n\n\n[ All vegetables ]\n무\n파프리카\n가지\n아보카도\n연근\n\n\n[ Fruits in korea ]\n무\n가지\n\n\n[ Fruits in US ]\n포도\n레몬\n\n\n[ All Yellow fruits ]\n레몬\n오렌지\n\n\n[ Black vegetables ]\n가지\n아보카도\n\n\n[ Fruits or Veges in red green color ]\n사과\n파프리카\n\n\n[ 8th li ]\n아보카도\n\n\n[ US product over the whole list ]\n포도\n레몬\n파프리카\n아보카도\n\n\n"
    }
   ],
   "source": [
    "html2=\"\"\"\n",
    "<html>\n",
    "<body>\n",
    "<div id=\"main-goods\" role=\"page\">\n",
    "<h1>과일과 야채</h1>\n",
    "<ul id=\"fr-list\">\n",
    "<li class=\"red green\" data-lo=\"ko\">사과</li>\n",
    "<li class=\"purple\" data-lo=\"us\">포도</li>\n",
    "<li class=\"yellow\" data-lo=\"us\">레몬</li>\n",
    "<li class=\"yellow\" data-lo=\"ko\">오렌지</li>\n",
    "</ul>\n",
    "<ul id=\"ve-list\">\n",
    "<li class=\"white green\" data-lo=\"ko\">무</li>\n",
    "<li class=\"red green\" data-lo=\"us\">파프리카</li>\n",
    "<li class=\"black\" data-lo=\"ko\">가지</li>\n",
    "<li class=\"black\" data-lo=\"us\">아보카도</li>\n",
    "<li class=\"white\" data-lo=\"cn\">연근</li>\n",
    "</ul>\n",
    "</div>\n",
    "<body>\n",
    "</html>\n",
    "\"\"\"\n",
    "soup = BeautifulSoup(html2, 'html.parser')\n",
    "sso = lambda ex : print(soup.select_one(ex).string)\n",
    "\n",
    "def prt(title, ex):\n",
    "    print(f'[ {title} ]')\n",
    "    for fruit in soup.select(ex):\n",
    "        print(fruit.string)\n",
    "    print('\\n')\n",
    "\n",
    "### 1. Get all fruits\n",
    "prt('All fruits', 'ul#fr-list li')\n",
    "### 2. Get all vegetables\n",
    "prt(\"All vegetables\", 'ul#ve-list li')\n",
    "### 3. Get vegetables in korea\n",
    "prt(\"Fruits in korea\", \"ul#ve-list li[data-lo='ko']\")\n",
    "### 4. Get fruits in US\n",
    "prt(\"Fruits in US\", \"ul#fr-list li[data-lo='us']\")\n",
    "### 5. Get all Yellow fruits\n",
    "prt(\"All Yellow fruits\",\"ul#fr-list li.yellow\")\n",
    "### 6. Get all Black vegetables \n",
    "prt(\"Black vegetables\",\"ul#ve-list li.black\")\n",
    "### 7. Get Fruits or Veges in red green color\n",
    "prt(\"Fruits or Veges in red green color\", \"div#main-goods li[class='red green']\")\n",
    "### 8. Get 8th li\n",
    "prt(\"8th li\", \"li:nth-of-type(8)\")\n",
    "### 9. Get US product over the whole list\n",
    "prt(\"US product over the whole list\", \"div#main-goods li[data-lo='us']\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weather forecast Data Web scaraping\n",
    "* Use function find(), find_all() from BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[부산ㆍ울산ㆍ경상남도]\n부산 (2020-07-25 00:00) : 흐리고 비 \n최저온도 : 22, 최고온도 : 26\n\n부산 (2020-07-25 12:00) : 흐리고 비 \n최저온도 : 22, 최고온도 : 26\n\n부산 (2020-07-26 00:00) : 흐리고 비 \n최저온도 : 22, 최고온도 : 26\n\n부산 (2020-07-26 12:00) : 흐림 \n최저온도 : 22, 최고온도 : 26\n\n부산 (2020-07-27 00:00) : 흐림 \n최저온도 : 22, 최고온도 : 27\n\n부산 (2020-07-27 12:00) : 흐리고 비 \n최저온도 : 22, 최고온도 : 27\n\n부산 (2020-07-28 00:00) : 흐리고 비 \n최저온도 : 23, 최고온도 : 27\n\n부산 (2020-07-28 12:00) : 흐리고 비 \n최저온도 : 23, 최고온도 : 27\n\n부산 (2020-07-29 00:00) : 흐리고 비 \n최저온도 : 23, 최고온도 : 28\n\n부산 (2020-07-29 12:00) : 흐리고 비 \n최저온도 : 23, 최고온도 : 28\n\n부산 (2020-07-30 00:00) : 흐림 \n최저온도 : 23, 최고온도 : 28\n\n부산 (2020-07-31 00:00) : 구름많음 \n최저온도 : 23, 최고온도 : 28\n\n부산 (2020-08-01 00:00) : 구름많음 \n최저온도 : 23, 최고온도 : 28\n\n"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests, os\n",
    "\n",
    "url = \"http://www.weather.go.kr/weather/forecast/mid-term-rss3.jsp\"\n",
    "res = requests.get(url)\n",
    "soup = BeautifulSoup(res.text, 'html.parser')\n",
    "\n",
    "for loca in soup.find_all('location'): # soup.find_all('body[wl_ver=\"3\"]')\n",
    "    prov = loca.find('province').string\n",
    "    city = loca.find('city').string\n",
    "    if city == '부산':\n",
    "        print(f'[{prov}]')\n",
    "        for data in loca.find_all('data'):\n",
    "            print(f'{city} ({data.tmef.string}) : {data.wf.string} \\n최저온도 : {data.tmn.string}, 최고온도 : {data.tmx.string}\\n')\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "using_attrs_in_find = soup.find('location', attrs = {'wl_ver' : '3'})\n",
    "# print(using_attrs_in_find)\n",
    "prov = using_attrs_in_find.find('province').text\n",
    "# print(prov)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make json file from weather forecast web (myself)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'\\nread json file\\n'"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests, os,json\n",
    "\n",
    "url = \"http://www.weather.go.kr/weather/forecast/mid-term-rss3.jsp\"\n",
    "res = requests.get(url)\n",
    "soup = BeautifulSoup(res.text, 'html.parser')\n",
    "\n",
    "\n",
    "infos = []\n",
    "for location in soup.find_all('location'): # soup.find_all('body[wl_ver=\"3\"]')\n",
    "    info = {}\n",
    "    info['province'] = location.province.string\n",
    "    info['city'] = location.city.string\n",
    "    info['data'] = []\n",
    "    for data in location.find_all('data'):\n",
    "        temp = {\n",
    "            'mode' : data.mode.string,\n",
    "            'tmef' : data.tmef.string,\n",
    "            'wf' : data.wf.string,\n",
    "            'tmn' : data.tmn.string,\n",
    "            'tmx' : data.tmx.string,\n",
    "            'rnst' : data.rnst.string\n",
    "        }\n",
    "        info['data'].append(temp)\n",
    "    infos.append(info)\n",
    "# print(infos)\n",
    "with open('my_weather.json', 'w', encoding= 'utf8') as f:\n",
    "    json.dump(infos, f, indent = 4, ensure_ascii=False) \n",
    "    '''\n",
    "    dump() function >> to make json file\n",
    "    endure_ascii = False >> can write korean alphabet in JSON file\n",
    "    indent = 4 >> to make json pretty\n",
    "    '''\n",
    "\n",
    "# with open('my_weather.json', 'r', encoding='utf8') as f:\n",
    "#     parsed = json.load(f)\n",
    "#     print(parsed)\n",
    "# '''\n",
    "# read json file\n",
    "# '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}