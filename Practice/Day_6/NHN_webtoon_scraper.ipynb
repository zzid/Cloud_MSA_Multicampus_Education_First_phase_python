{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. NHN webtoon image download and upload\n",
    "* Set the url to Header name 'Referer'\n",
    "* Using get() function from requests, request the image data\n",
    "* Using response.content property\n",
    "* Store as file in local"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Download images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests, os\n",
    "# print(os.getcwd())\n",
    "req_hearder = {\n",
    "    'referer' : 'https://comic.naver.com/webtoon/detail.nhn?titleId=746535&no=3',\n",
    "    # 'referer' is reserved header name!\n",
    "}\n",
    "\n",
    "img_urls =[\n",
    "    'https://image-comic.pstatic.net/webtoon/746535/3/20200720100426_690986752102510f0a4f0c65118d2681_IMAG01_1.jpg',\n",
    "'https://image-comic.pstatic.net/webtoon/746535/3/20200720100426_690986752102510f0a4f0c65118d2681_IMAG01_2.jpg',\n",
    "'https://image-comic.pstatic.net/webtoon/746535/3/20200720100426_690986752102510f0a4f0c65118d2681_IMAG01_3.jpg',\n",
    "\n",
    "]\n",
    "\n",
    "for img_url in img_urls:\n",
    "    res = requests.get(img_url, headers = req_hearder)\n",
    "    # print(res.ok)\n",
    "    # print(res.content)\n",
    "    # print(res.text)\n",
    "    img_data = res.content\n",
    "    file_name = os.path.basename(img_url)\n",
    "    print(file_name)\n",
    "    with open('./img/' + file_name, 'wb') as f:\n",
    "        print(f'Writing image to {file_name} ({len(img_data)} bytes)')\n",
    "        f.write(img_data)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Upload images\n",
    "* File upload have to use \"POST\" method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "upload_files ={\n",
    "    'img1' : open('./img/20200720100426_690986752102510f0a4f0c65118d2681_IMAG01_1.jpg', 'rb'),\n",
    "    'img2' : open('./img/20200720100426_690986752102510f0a4f0c65118d2681_IMAG01_2.jpg', 'rb'),\n",
    "    'img3' : open('./img/20200720100426_690986752102510f0a4f0c65118d2681_IMAG01_3.jpg', 'rb')\n",
    "}\n",
    "url = 'https://httpbin.org/post'\n",
    "res = requests.post(url, files = upload_files)\n",
    "# print(res.status_code)\n",
    "with open('./img/' + 'url', 'w') as f:\n",
    "    f.write(res.json()['files']['img3'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Download whole image from one webtoon page\n",
    "* soup.select('img\\[src$=.jpg\\]')\n",
    "* Make img dir, store images to that dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests,os,re\n",
    "\n",
    "main_url = 'https://comic.naver.com/webtoon/detail.nhn?titleId=746535&no=3'\n",
    "\n",
    "req_hearder = {\n",
    "    'referer' : main_url\n",
    "    # 'referer' is reserved header name!\n",
    "}\n",
    "res = requests.get(main_url)\n",
    "html = res.text\n",
    "\n",
    "soup = BeautifulSoup(html, 'html.parser') # make object first\n",
    "img_urls = []\n",
    "\n",
    "if not os.path.isdir('./img'):\n",
    "    os.mkdir('./img')\n",
    "\n",
    "for i in soup.select('img[src$=.jpg]'):\n",
    "    # img_urls.append(img_url.get('src'))\n",
    "    img_url = i.get('src')\n",
    "    res = requests.get(img_url, headers = req_hearder)\n",
    "    img_data = res.content\n",
    "    file_name = os.path.basename(img_url)\n",
    "    with open('./img/' + file_name, 'wb') as f:\n",
    "        print(f'Writing image to {file_name} ({len(img_data)} bytes)')\n",
    "        f.write(img_data)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os, requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "url = \"https://comic.naver.com/index.nhn\"\n",
    "res = requests.get(url)\n",
    "req_hearder ={\n",
    "    'referer' : url,\n",
    "}\n",
    "text = res.text\n",
    "# print(text)\n",
    "soup = BeautifulSoup(text, 'html.parser')\n",
    "# print(soup)\n",
    "recomanded = {}\n",
    "for i in soup.select(\".genreRecomImg2\"):\n",
    "    link =''\n",
    "    for a in i.select('a'):\n",
    "        link = urljoin(url, a.get('href'))\n",
    "        # print(link)\n",
    "    for j in i.select(\"img[src$=.jpg]\"):\n",
    "        img_src = j.get('src')\n",
    "        ttl = j.get('title')\n",
    "        if not os.path.isdir(\"./recomToon\"):\n",
    "            os.mkdir(\"./recomToon\")\n",
    "        with open('./recomToon/' + ttl +'.jpg', 'wb') as f:\n",
    "            res = requests.get(img_src, headers = req_hearder)\n",
    "            img_data = res.content\n",
    "            f.write(img_data)\n",
    "        recomanded[ttl] = link\n",
    "print(recomanded)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests, html\n",
    "def wrt(soupObject,url):\n",
    "    with open(url,'w') as f:\n",
    "        f.write(soupObject.prettify(formatter='html'))\n",
    "with open('./test2.html', encoding='utf8') as text:\n",
    "    soup = BeautifulSoup(text)\n",
    "    # print(soup)\n",
    "    og_tag = soup.body\n",
    "    n_tag = soup.new_tag('div', id = 'test')\n",
    "    og_tag.append(n_tag)\n",
    "    n_tag.string = 'Hi! i\\'m in2!'\n",
    "    soup.body.insert(3, n_tag)\n",
    "    wrt(soup, './test.html')\n",
    "# li_tag.i.replace_with(new_tag)\n",
    "\n",
    "# html_txt = soup.prettify(formatter='html')\n",
    "# text.close\n",
    "# print(html_txt)\n",
    "# with open('test.html', 'w') as file2:\n",
    "#     file2.write(html_txt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Will do again later \n",
    "'''\n",
    "# import os, requests\n",
    "# from bs4 import BeautifulSoup\n",
    "# from urllib.parse import urljoin\n",
    "# url = \"https://comic.naver.com/index.nhn\"\n",
    "# res = requests.get(url)\n",
    "# req_hearder ={\n",
    "#     'referer' : url,\n",
    "# }\n",
    "# text = res.text\n",
    "# # print(text)\n",
    "# soup = BeautifulSoup(text, 'html.parser')\n",
    "# # print(soup)\n",
    "# recomanded = {}\n",
    "# for i in soup.select(\".genreRecomImg2\"):\n",
    "#     link =''\n",
    "#     for a in i.select('a'):\n",
    "#         link = urljoin(url, a.get('href'))\n",
    "#         # print(link)\n",
    "#     for j in i.select(\"img[src$=.jpg]\"):\n",
    "#         img_src = j.get('src')\n",
    "#         ttl = j.get('title')\n",
    "#         res = requests.get(img_src, headers = req_hearder)\n",
    "#         img_data = res.content\n",
    "#         soup2 = BeautifulSoup(f)\n",
    "#         og_tag = soup2.body\n",
    "#         new_tag = soup2.new_tag('img', src = img_data)\n",
    "#         og_tag.append(new_tag)\n",
    "#         soup2.body.insert(3,new_tag)\n",
    "#         with open('test2.html', 'w') as f:\n",
    "#             f.write(soup2.prettify(formatter='html'))\n",
    "#         recomanded[ttl] = link\n",
    "# print(recomanded)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Make function, can download images from each pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "import os, requests,re\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.replace(\"\\n\", \"\")\n",
    "    cleaned_text = re.sub('[a-zA-Z]', '', text)\n",
    "    cleaned_text = re.sub('[\\{\\}\\[\\]\\/?.,;:|\\)*~`!^\\-_+<>@\\#$%&\\\\\\=\\(\\'\\\"]','', cleaned_text)\n",
    "    return cleaned_text\n",
    "\n",
    "def get_recom_toons(url):\n",
    "    res = requests.get(url)\n",
    "    soup = BeautifulSoup(res.text, 'html.parser')\n",
    "    toon_list = {}\n",
    "    for i in soup.select('#genreRecommand'):\n",
    "        for j in i.select('a[href^=\"/webtoon/list\"]'):\n",
    "            link = urljoin(url, j.get('href'))\n",
    "            for k in j.select('img'):\n",
    "                toon_list[k.get('title')] = link\n",
    "            # print(j.get('href'))\n",
    "            # print(j.a.string)\n",
    "    print(toon_list)\n",
    "\n",
    "    for i in toon_list:\n",
    "        title = clean_text(i)\n",
    "        if not os.path.isdir(title):\n",
    "            os.mkdir(title)\n",
    "        \n",
    "        with open(i + '/' +'links.txt', 'w', encoding='utf8') as f:\n",
    "            f.write(toon_list[i])\n",
    "    \n",
    "    return toon_list\n",
    "    # print(titles)\n",
    "    # if not os.path.isdir(dirname):\n",
    "    #     os.mkdir(dirname)\n",
    "\n",
    "def make_list_page(dirname,url):\n",
    "    res = requests.get(url)\n",
    "    text = res.text\n",
    "    # print(text)\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "    links = {}\n",
    "    for a in soup.select('#content'):\n",
    "        for i in a.select('a[href^=\"/webtoon/detail\"]'):\n",
    "            full_url = urljoin(url, i.get('href'))\n",
    "            if i.string:\n",
    "                links[i.string] = full_url\n",
    "        # print(links)\n",
    "        with open(dirname + '/' +'links.txt', 'w', encoding='utf8') as f:\n",
    "            for i in links:\n",
    "                # f.write(links[i] + '\\t\\t' + i + '\\n')\n",
    "                f.write(links[i] +'\\n')\n",
    "\n",
    "def go_webtoon(url,title):\n",
    "    url = url[:len(url)-1] # delete '\\n'\n",
    "    res =requests.get(url)\n",
    "    req_hearder ={\n",
    "        'referer' : url,\n",
    "    }\n",
    "    soup = BeautifulSoup(res.text, 'html.parser')\n",
    "    file_name = soup.h3.string\n",
    "    file_name = clean_text(file_name)\n",
    "    dir_name = title + '/' + file_name\n",
    "    if not os.path.isdir(dir_name):\n",
    "        os.mkdir(dir_name)\n",
    "    for a in soup.select('.wt_viewer'):\n",
    "        for b in a.select('img'):\n",
    "            img_src = b.get('src')\n",
    "            # print(img_src)\n",
    "            res = requests.get(img_src, headers = req_hearder)\n",
    "            img_data = res.content\n",
    "            # print(img_data)\n",
    "            file_name = os.path.basename(img_src)\n",
    "            print(file_name)\n",
    "            with open(dir_name +'/' + file_name, 'wb') as f:\n",
    "                # print(f'Writing image to {file_name} ({len(img_data)} bytes)')\n",
    "                f.write(img_data)\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    root = \"https://comic.naver.com/index.nhn\"\n",
    "    title_list = get_recom_toons(root)\n",
    "    for title in title_list:\n",
    "        make_list_page(title, title_list[title])\n",
    "        with open(title + '/' + 'links.txt', 'r') as f:\n",
    "            while 1:\n",
    "                line = f.readline()\n",
    "                if not line:\n",
    "                    break\n",
    "                go_webtoon(line,title)\n",
    "\n",
    "\n",
    "main()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}